{# Untagged, Tokenized Output #}

{# Are there untagged tokens between the end of tag_map and the start of next_tag_map? #}
{% if
    tokens is defined and
    s is defined and
    untagged_tokens_index_start is defined and
    untagged_tokens_index_end is defined and
    tokens is defined and
    untagged_tokens_index_start < tokens|length and
    untagged_tokens_index_end < tokens|length and
    untagged_tokens_index_end + 1 - untagged_tokens_index_start > 0
%}
    {# TODO: Output untokenized chars BEFORE first token!!! #}

    {# We have untagged tokens, so let's output 'em. #}
    {% for token_index in range(untagged_tokens_index_start, untagged_tokens_index_end + 1) %}
        {% set token = tokens[token_index] %}
        {% if token.TYPE == token_types["WHITESPACE"] or token.TYPE == token_types["NEWLINE"] %}
            {% set token_str = token.STRS[token_whitespace_newline_str_to_output_index] %}
        {% else %}
            {% set token_str = token.STRS[token_str_to_output_index] %}
        {% endif %}
{#        <h2>tokens[{{ untagged_tokens_index_start }}:{{ untagged_tokens_index_end + 1 }}]</h2>#}
        <span id="{{ token.POS }}" class="token">{{ token_str }}</span>

        {% set next_pos = token.POS + token.LENGTH %}

        {# TODO: Fix missing str output #}

        {# There could be untokenized chars in between these tokens! #}
        {% set next_token_index = token_index + 1 %}
        {% if next_token_index < (untagged_tokens_index_end + 1) %}
            {% set next_token = tokens[next_token_index] %}
            {# Untokenized char byte positions, i.e. indexes into the str, s.#}
            {% set untokenized_chars_pos_start = next_pos %}
            {% set untokenized_chars_pos_end = next_token.POS - 1 %}

            {% include "__s.html" %}
        {% endif %}
    {% endfor %}
{% endif %}
